{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appuie pour faire tourner les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modules à installer se trouvent dans requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer les repertoires outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_repertoires():\n",
    "    reps = ['output', 'output/clusters', 'cluster_links', 'outputgml', 'outputLA']\n",
    "    for rep in reps:\n",
    "        os.makedirs(rep, exist_ok=True)\n",
    "    print(\"Répertoires crées.\")\n",
    "\n",
    "# Call the function to create the directories\n",
    "creer_repertoires()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content_path, links_path):\n",
    "    if not os.path.exists(content_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{content_path}' est introuvable.\")\n",
    "    with open(content_path, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "    if not os.path.exists(links_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{links_path}' est introuvable.\")\n",
    "    with open(links_path, 'r', encoding='utf-8') as file:\n",
    "        links = json.load(file)\n",
    "    \n",
    "    return content, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(data, filename):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        np.savetxt(f'output/{filename}.txt', data, fmt='%.4f')\n",
    "    elif isinstance(data, dict):\n",
    "        with open(f'output/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    elif isinstance(data, list):\n",
    "        with open(f'output/{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in data:\n",
    "                f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON TO GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_gml (links_path, output_file):\n",
    "    with open(links_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        links_path = json.load(file)\n",
    "    noeuds = set()\n",
    "    aretes = []\n",
    "    for source, cible in links_path.items():\n",
    "        noeuds.add(source)\n",
    "        for cible in cible:\n",
    "            noeuds.add(cible)\n",
    "            aretes.append((source, cible))\n",
    "    noeuds_id = {}\n",
    "    id = 0\n",
    "    for noeud in noeuds:\n",
    "        noeuds_id[noeud] = id\n",
    "        id += 1\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"graph\\n\")\n",
    "        file.write(\"[\\n\")\n",
    "        file.write(\"  directed 1\\n\")\n",
    "        #noeuds\n",
    "        for noeud, id in noeuds_id.items():\n",
    "            file.write(\"  node\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    id {id}\\n\")\n",
    "            file.write(f\"    label \\\"{noeud}\\\"\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "        #arêtes\n",
    "        for source, cible in aretes:\n",
    "            file.write(\"  edge\\n\")\n",
    "            file.write(\"  [\\n\")\n",
    "            file.write(f\"    source {noeuds_id[source]}\\n\")\n",
    "            file.write(f\"    target {noeuds_id[cible]}\\n\")\n",
    "            file.write(\"  ]\\n\")\n",
    "        file.write(\"]\\n\")\n",
    "    print(f\"Fichier GML créé : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement du contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [STEMMER.stem(token) for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tokens(content, output_file):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    for page_title, page_content in content.items():\n",
    "        tokens = preprocess_text(page_content)\n",
    "        cleaned_data[page_title] = tokens\n",
    "    save_to_file(cleaned_data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de texte spécifique à SIA, pour éviter de supprimer les \"not\" et autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SIA(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for content in corpus.values():\n",
    "        tokens.extend(preprocess_text(content))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement des modèles tfidfj et doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle Doc2Vec\n",
    "def train_doc2vec_model(documents, vector_size=100, window=1, epochs=20):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(tagged_data, vector_size=vector_size, window=2, min_count=1, workers=4, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# Entrainement du modèle TFIDF\n",
    "def train_tfidf_model(documents):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les phrases comprenant un ou des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(corpus, words):\n",
    "    sentences_with_words = []\n",
    "    words_stem = [STEMMER.stem(word.lower()) for word in words]\n",
    "    \n",
    "    for content in corpus.values():\n",
    "        sentences = sent_tokenize(content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens =word_tokenize(sentence.lower())\n",
    "            stemmed_tokens =[STEMMER.stem(token) for token in tokens]\n",
    "            if all(word in stemmed_tokens for word in words_stem):\n",
    "                sentences_with_words.append(sentence)\n",
    "    \n",
    "    return sentences_with_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments sur des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_sentiment(sentences):\n",
    "    sia =SentimentIntensityAnalyzer()\n",
    "    sentiment_scores ={\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    for sentence in sentences:\n",
    "        scores =sia.polarity_scores(sentence)\n",
    "        sentiment_scores[\"positive\"] += scores[\"pos\"]\n",
    "        sentiment_scores[\"negative\"] += scores[\"neg\"]\n",
    "        sentiment_scores[\"neutral\"] += scores[\"neu\"]\n",
    "        sentiment_scores[\"compound\"] += scores[\"compound\"]\n",
    "    # Moyenne des scores\n",
    "    num_sentences =len(sentences)\n",
    "    if num_sentences > 0:\n",
    "        sentiment_scores ={key: value / num_sentences for key, value in sentiment_scores.items()}\n",
    "    # Déterminer le sentiment principal\n",
    "    if sentiment_scores[\"positive\"] > sentiment_scores[\"negative\"] and sentiment_scores[\"positive\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment =\"Positif\"\n",
    "    elif sentiment_scores[\"negative\"] > sentiment_scores[\"positive\"] and sentiment_scores[\"negative\"] > sentiment_scores[\"neutral\"]:\n",
    "        sentiment =\"Négatif\"\n",
    "    else:\n",
    "        sentiment =\"Neutre\"\n",
    "\n",
    "    return sentiment_scores, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinaison d'analyse de sentiments et de trouver les phrases avec un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_word_sentiment(content, word):\n",
    "    print(word)\n",
    "    #Trouver les phrases contenant le mot\n",
    "    sentences_with_word=find_sentences_with_words(content, word)\n",
    "    print(sentences_with_word)\n",
    "    if not sentences_with_word:\n",
    "        print(f\"Le mot '{word}' n'a pas été trouvé dans le corpus.\")\n",
    "        return {\"word\": word,\"sentences_with_word\": [], \"sentiment_scores\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0},\"overall_sentiment\": \"Aucun (mot non trouvé)\"}\n",
    "    # Analyser le sentiment des phrases\n",
    "    sentiment_scores, sentiment = analyze_word_sentiment(sentences_with_word)\n",
    "\n",
    "    return {\"word\": word,\"sentences_with_word\": sentences_with_word,\"sentiment_scores\": sentiment_scores,\"overall_sentiment\": sentiment}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments sur une page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def page_sentiment(page_name, content):\n",
    "    if page_name not in content:\n",
    "        return f\"La page '{page_name}' n'existe pas dans le corpus.\"\n",
    "    page_content=content[page_name]\n",
    "    sentences=sent_tokenize(page_content)\n",
    "    sentiment_scores,general_sent =analyze_word_sentiment(sentences)\n",
    "\n",
    "    # Ajout pour cas neutre avec consonance\n",
    "    if general_sent== \"Neutre\":\n",
    "        if sentiment_scores[\"positive\"]>sentiment_scores[\"negative\"]:\n",
    "            general_sent = \"Neutre avec consonance plus positive\"\n",
    "        elif sentiment_scores[\"negative\"]>sentiment_scores[\"positive\"]:\n",
    "            general_sent= \"Neutre avec consonance plus négative\"\n",
    "\n",
    "    return {\"page_name\": page_name,\"sentiment_scores\": sentiment_scores,\"overall_sentiment\": general_sent}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments sur un ensemble de page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_corpus_sentiment(corpus):\n",
    "    general_sent = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"compound\": 0}\n",
    "    document_sentiments = {}\n",
    "    for doc_name, doc_content in corpus.items():\n",
    "        sentences = sent_tokenize(doc_content)\n",
    "        sentiment_scores, overall_sentiment = analyze_word_sentiment(sentences)\n",
    "        document_sentiments[doc_name] = {\"sentiment_scores\": sentiment_scores,\"overall_sentiment\": overall_sentiment}\n",
    "\n",
    "        #Accumuler les scores de sentiment pour l'ensemble du corpus\n",
    "        for key in general_sent:\n",
    "            general_sent[key] += sentiment_scores[key]\n",
    "    # Moyenne des scores pour l'ensemble du corpus\n",
    "    num_documents = len(corpus)\n",
    "    if num_documents > 0:\n",
    "        general_sent = {key: value / num_documents for key, value in general_sent.items()}\n",
    "\n",
    "    return {\"overall_sentiment_scores\": general_sent,\"document_sentiments\": document_sentiments}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cooccurrences(corpus_path, keyword, min_freq=2, window_size=5):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = json.load(file)\n",
    "    all_tokens = [] \n",
    "    for text in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(text))\n",
    "    #Trouver les bigrammes\n",
    "    bigram_measures= BigramAssocMeasures()\n",
    "    finder=BigramCollocationFinder.from_words(all_tokens, window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    \n",
    "    #Filtrer les bigrammes avce le mot-clé\n",
    "    keyword_stem=STEMMER.stem(keyword)\n",
    "    bigrams={}\n",
    "    for bigram, freq in finder.ngram_fd.items():\n",
    "        if keyword_stem in bigram:\n",
    "            distances=[]\n",
    "            for i in range(len(all_tokens) - 1):\n",
    "                if all_tokens[i]==bigram[0] and bigram[1] in all_tokens[i + 1:i + window_size + 1]:\n",
    "                    j=all_tokens.index(bigram[1], i + 1, i + window_size + 1)\n",
    "                    distances.append(abs(j - i))\n",
    "            bigrams[bigram]={\"frequency\": freq,\"mean_distance\": sum(distances) / len(distances) if distances else 0}\n",
    "    #Trier les résultats\n",
    "    sorted_bigrams=sorted(bigrams.items(), key=lambda item: item[1]['frequency'], reverse=True)\n",
    "    print(f\"Cooccurrences pour le mot-clé {keyword}:\\n\")\n",
    "    print(f\"{'Cooccurrence':<20}\\t{'Frequency':<10}\\t{'Mean Distance':<15}\")\n",
    "    print(f\"{'-' * 50}\")\n",
    "    for bigram, data in sorted_bigrams:\n",
    "        print(f\"{bigram[0]} {bigram[1]:<17}\\t{data['frequency']:<10}\\t{data['mean_distance']:<15.2f}\")\n",
    "    return sorted_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trigram_cooccurrences(corpus_path, keyword, min_freq=2, window_size=5):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus=json.load(file)\n",
    "    all_tokens=[]\n",
    "    for text in corpus.values():\n",
    "        all_tokens.extend(preprocess_text(text))\n",
    "    trigram_measures =nltk.collocations.TrigramAssocMeasures()\n",
    "    finder =nltk.collocations.TrigramCollocationFinder.from_words(all_tokens, window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    # Filtrer les trigrammes contenant le mot-clé\n",
    "    keyword_stem=STEMMER.stem(keyword)\n",
    "    trigrams={}\n",
    "    for trigram, freq in finder.ngram_fd.items():\n",
    "        if keyword_stem in trigram:\n",
    "            distances=[]\n",
    "            for i in range(len(all_tokens) - 2):\n",
    "                if all_tokens[i] == trigram[0] and trigram[1] in all_tokens[i + 1:i + window_size + 1] and trigram[2] in all_tokens[i + 2:i + window_size + 2]:\n",
    "                    j=all_tokens.index(trigram[1], i + 1, i + window_size + 1)\n",
    "                    k=all_tokens.index(trigram[2], i + 2, i + window_size + 2)\n",
    "                    distances.append(abs(k - i))\n",
    "            trigrams[trigram]={\"frequency\": freq,\"mean_distance\": sum(distances) / len(distances) if distances else 0} \n",
    "    # Trier les résultats\n",
    "    sorted_trigrams=sorted(trigrams.items(), key=lambda item: item[1]['frequency'], reverse=True)\n",
    "    # Afficher les résultats\n",
    "    print(f\"Cooccurrences pour le mot-clé '{keyword}':\\n\")\n",
    "    print(f\"{'Cooccurrence':<30}\\t{'Frequency':<10}\\t{'Mean Distance':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for trigram, data in sorted_trigrams:\n",
    "        print(f\"{trigram[0]} {trigram[1]} {trigram[2]:<17}\\t{data['frequency']:<10}\\t{data['mean_distance']:<15.2f}\")\n",
    "    return sorted_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(corpus, n=20):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    return freq_dist.most_common(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(corpus, output_filename='wordcloud.png'):\n",
    "    all_tokens = tokenize_corpus(corpus)\n",
    "    text = ' '.join(all_tokens)\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=30, min_font_size=10).generate(text)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'output/{output_filename}', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau des tops tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_tableau(json_path, output_csv):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content=json.load(file)\n",
    "    token_counts={} #Dictionnaire pour les occurrences\n",
    "    document_counts=defaultdict(int)  #Nombre de documents contenant chaque token\n",
    "    mots_assoc=defaultdict(set)  #Mots originaux associés aux tokens stemmés\n",
    "    for text in content.values():\n",
    "        tokens=word_tokenize(text.lower())\n",
    "        stemmed_tokens=[STEMMER.stem(token) for token in tokens if token.isalnum() and token not in STOP_WORDS]\n",
    "\n",
    "        #Compter les occurrences globales\n",
    "        for token in stemmed_tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token]+= 1\n",
    "            else:\n",
    "                token_counts[token]=1\n",
    "\n",
    "        #Compter les documents contenant chaque token (uniquement une fois par document)\n",
    "        unique_tokens = set(stemmed_tokens)\n",
    "        for token in unique_tokens:\n",
    "            document_counts[token]+=1\n",
    "\n",
    "        #Garder les mots originaux aux tokens stemmés\n",
    "        for word in tokens:\n",
    "            if word.isalnum() and word not in STOP_WORDS:\n",
    "                stemmed = STEMMER.stem(word)\n",
    "                mots_assoc[stemmed].add(word)\n",
    "\n",
    "    # Construire et sauvegarder le tableau en une étape\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Token (stemmatisé)\", \"Mots associés\", \"Occurrences\", \"Articles concernés\"])\n",
    "        # Écrire les données\n",
    "        for token, freq in token_counts.items():\n",
    "            writer.writerow([token,', '.join(mots_assoc[token]),freq,document_counts[token]])\n",
    "    print(f\"Tableau sauvegardé dans {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de similarité entre 2 documents au choix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(model, doc_id1, doc_id2):\n",
    "    vec1 = model.dv[doc_id1]\n",
    "    vec2 = model.dv[doc_id2]\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche de documents similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_docs_tfidf(content, tfidf_matrix, title, n=4):\n",
    "    if title not in content:\n",
    "        raise ValueError(f\"Le titre '{title}' n'existe pas dans le contenu.\")\n",
    "\n",
    "    doc_index = list(content.keys()).index(title)\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[doc_index], tfidf_matrix).flatten()\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:n+1]\n",
    "    similar_docs = [(list(content.keys())[i], cosine_similarities[i]) for i in similar_indices]\n",
    "    output_file = os.path.join(f\"output/{title}.txt\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Document de base: {title}\\n\")\n",
    "        file.write(content[title] + \"\\n\\n\")\n",
    "        \n",
    "        for doc_title, similarity in similar_docs:\n",
    "            file.write(f\"Document similaire: {doc_title} (Similarité: {similarity:.4f})\\n\")\n",
    "            file.write(content[doc_title] + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Documents sauvegardés dans {output_file}\")\n",
    "    return similar_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de similarité par cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(content, model):\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = np.array([model.dv[i] for i in range(len(content))]) \n",
    "    # Calcul de la matrice de similarité par le cosinus\n",
    "    similarity_matrix = cosine_similarity(doc_vectors)\n",
    "    # Sauvegarder la matrice de similarité dans un fichier\n",
    "    save_to_file(similarity_matrix, 'similarity_matrix.json') \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(vectors, max_clusters=10, min_clusters=2):\n",
    "    distortions=[]\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans =KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(vectors)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(min_clusters, max_clusters + 1), distortions, marker='o')\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('Méthode du coude pour trouver le nombre optimal de clusters')\n",
    "    plt.show()\n",
    "    optimal_k =min_clusters\n",
    "    for i in range(1, len(distortions) - 1):\n",
    "        if distortions[i] - distortions[i + 1] < distortions[i - 1] - distortions[i]:\n",
    "            optimal_k=min_clusters + i\n",
    "            break\n",
    "    print(f\"Nombre optimal de clusters : {optimal_k}\")\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents(content, optimal_k):\n",
    "\n",
    "    # Préparer les documents\n",
    "    documents = list(content.values())\n",
    "\n",
    "    # Entraîner le modèle Doc2Vec\n",
    "    model = train_doc2vec_model(documents, vector_size=100, window=2, epochs=40)\n",
    "\n",
    "    # Obtenir les vecteurs des documents\n",
    "    doc_vectors = [model.dv[i] for i in range(len(content))]\n",
    "\n",
    "    # Appliquer KMeans\n",
    "    kmeans =KMeans(n_clusters=optimal_k, max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(doc_vectors)\n",
    "\n",
    "    clusters = {}\n",
    "    for i, doc in enumerate(doc_vectors):\n",
    "        cluster_label = int(kmeans.labels_[i])\n",
    "        document_key = list(content.keys())[i]  # Utilisation de la clé du dictionnaire\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(document_key)\n",
    "\n",
    "    # Sauvegarder les clusters\n",
    "    save_to_file(clusters, 'clusters')\n",
    "    \n",
    "    # Créer un fichier texte avec les titres des documents regroupés par clusters\n",
    "    with open('output/clusters_titles.txt', 'w', encoding='utf-8') as f:\n",
    "        for cluster_label, docs in clusters.items():\n",
    "            f.write(f\"Cluster {cluster_label}:\\n\")\n",
    "            for doc in docs:\n",
    "                f.write(f\"- {doc}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Calcul et sauvegarde de la matrice de similarité\n",
    "    similarity_matrix= calculate_cosine_similarity(content, model)\n",
    "\n",
    "    return clusters, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde les titres complets des documents dans chaque cluster dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_individually(clusters, content_path):\n",
    "    with open(content_path, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "    # Créer un dossier pour stocker les fichiers des clusters\n",
    "    os.makedirs('output/clusters', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder chaque cluster dans un fichier JSON séparé\n",
    "    for num_cluster, docs in clusters.items():\n",
    "        cluster_content = {doc: content[doc] for doc in docs}  # Documents de ce cluster\n",
    "        output_file = f'output/clusters/cluster_{num_cluster}.json'  # Nom du fichier JSON\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Cluster {num_cluster} sauvegardé dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links par clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_per_cluster(clusters_dir, original_links_path, output_dir):\n",
    "    with open(original_links_path, 'r', encoding='utf-8') as f:\n",
    "        original_links = json.load(f)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for cluster_file in os.listdir(clusters_dir):\n",
    "        cluster_path = os.path.join(clusters_dir, cluster_file)\n",
    "        with open(cluster_path, 'r', encoding='utf-8') as f:\n",
    "            cluster_nodes = json.load(f)\n",
    "        cluster_links = {}\n",
    "        for node in cluster_nodes:\n",
    "            if node in original_links:\n",
    "                cluster_links[node] = [target for target in original_links[node] if target in cluster_nodes]\n",
    "\n",
    "        cluster_label = os.path.splitext(cluster_file)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{cluster_label}_links.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cluster_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fichier de liens créé pour le cluster : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisis ce que tu veux lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemin du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    content_path = 'contentB.json'\n",
    "    links_path = 'linksB.json'\n",
    "    content, links = load_data(content_path, links_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entrainer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_doc2vec_model(content, vector_size=100, window=5, epochs=50)\n",
    "tfidf_matrix = train_tfidf_model(content)\n",
    "doc_vectors = ([model.dv[i] for i in range(len(content))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases content un/des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_find = ['diversity', 'inclusion', 'equity']\n",
    "sentences = find_sentences_with_words(content, words_to_find)\n",
    "with open('output/sentences_with_words.txt', 'w', encoding='utf-8') as file:\n",
    "    pass\n",
    "with open('output/sentences_with_words.txt', 'w', encoding='utf-8') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiment pour des phrases contenant des mots choisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_word_sentiment(content, [\"performance\", \"inclusion\", \"communication\"])\n",
    "\n",
    "print(f\"Analyse de sentiment pour le mot '{result['word']}':\")\n",
    "print(f\"Scores moyens : {result['sentiment_scores']}\")\n",
    "print(f\"Sentiment global : {result['overall_sentiment']}\")\n",
    "print(\"\\nExemples de phrases contenant le mot :\")\n",
    "for sentence in result[\"sentences_with_word\"][:5]:  # Afficher jusqu'à 5 phrases\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments pour 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse de sent. 1 page\n",
    "page_name = \"Minimum wage\"\n",
    "page_sentiment(page_name, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de sentiments pour un ensemble de page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse de sent. corpus\n",
    "corpus = \"outputidm/cooperative.txt\"\n",
    "acs = analyze_corpus_sentiment(corpus)\n",
    "print(acs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'cleaned_data'\n",
    "clean_and_save_tokens(content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'contentB.json'  \n",
    "keyword = \"white\"         \n",
    "find_cooccurrences(corpus_path, keyword, min_freq=47, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'contentB.json'  \n",
    "keyword = \"white\"         \n",
    "find_trigram_cooccurrences(corpus_path, keyword, min_freq=10, window_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = show_top_tokens(content)\n",
    "print(\"Top Tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tokens Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'output/token_table.csv'\n",
    "token_tableau(content_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarité entre 2 docs /avec 1 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id1 = '0'\n",
    "doc_id2 = '1'\n",
    "similarity = calculate_similarity(model, doc_id1, doc_id2)\n",
    "print(\"Similarité entre doc 0 et doc 1 :\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les documents similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Cooperative\"\n",
    "similar_docs_tfidf = find_similar_docs_tfidf(content, tfidf_matrix, title)\n",
    "print(f\"Documents similaires à '{title}':\")\n",
    "for doc, similarity in similar_docs_tfidf:\n",
    "    print(f\"- {doc} (similarité: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = StandardScaler().fit_transform(doc_vectors)\n",
    "tfidf_vectors = StandardScaler().fit_transform(tfidf_matrix.toarray())\n",
    "combined_vectors = np.hstack((doc_vectors, tfidf_matrix.toarray()))\n",
    "\n",
    "optimal_k = find_optimal_clusters(combined_vectors, max_clusters=10, min_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer les clusters individuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, similarity_matrix = cluster_documents(content, optimal_k)\n",
    "save_clusters_individually(clusters, \"contentB.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer des liens pour chaque cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dir = \"output/clusters\"\n",
    "original_links_path = \"linksB.json\"\n",
    "output_dir = \"cluster_links\"\n",
    "\n",
    "create_links_per_cluster(clusters_dir, original_links_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connaitre le nombre de doc par cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_label, docs in clusters.items():\n",
    "    print(f\"Cluster {cluster_label} contains {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = \"linksB.json\"\n",
    "output_file = \"outputgml/graph.gml\"\n",
    "json_to_gml(links, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph pour les tokens individuels, pour les clusters // NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file1 = \"outputgml/tokens_graph.gml\"\n",
    "output_file2 = \"outputgml/clusters_graph.gml\"\n",
    "tokens_file = \"output/cleaned_data.json\"\n",
    "clusters_file = \"output/clusters.json\"\n",
    "json_to_gml(tokens_file, output_file1)\n",
    "json_to_gml(clusters_file, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph pour chaque cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = find_optimal_clusters(combined_vectors, max_clusters=14, min_clusters=4)\n",
    "for i in range(0,num_clusters):\n",
    "    json_to_gml(f\"cluster_links/cluster_{i}_links.json\", f\"outputgml/cluster_{i}.gml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
